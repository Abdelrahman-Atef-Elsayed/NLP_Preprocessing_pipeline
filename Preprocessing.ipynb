{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7762c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clean Tokens: ['hello', 'testing', 'nlp', 'pipeline', 'using', 'sample', 'url', 'along', 'contraction', 'like', 'number', '123']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ“Œ NLP Preprocessing Pipeline (Reusable Python Template)\n",
    "---------------------------------------------------------\n",
    "This script provides a general-purpose text preprocessing pipeline for common NLP tasks, including:\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Topic modeling\n",
    "- Named entity recognition\n",
    "\n",
    "Usage:\n",
    "------\n",
    "Import `preprocess_text()` into your project, or run this script directly to test on a sample text.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# ðŸ”§ Download required NLTK resources (only first time)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "# âš™ï¸ Initialize tools\n",
    "_stopwords = set(stopwords.words(\"english\"))\n",
    "_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Preprocesses raw input text through a standard NLP pipeline.\n",
    "\n",
    "    Steps:\n",
    "        1. Remove HTML tags\n",
    "        2. Convert to lowercase\n",
    "        3. Expand contractions\n",
    "        4. Remove URLs\n",
    "        5. Remove punctuation and special characters\n",
    "        6. Normalize whitespace\n",
    "        7. Tokenize text\n",
    "        8. Remove stopwords\n",
    "        9. Lemmatize tokens\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw input text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of clean, lemmatized tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # 4. Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # 5. Remove punctuation and special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # 6. Normalize extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 7. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 8. Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in _stopwords]\n",
    "\n",
    "    # 9. Lemmatize tokens\n",
    "    tokens = [_lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ðŸš€ Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    <p>Hello there! I'm testing this NLP pipeline using a sample URL: https://example.com,\n",
    "    along with contractions like can't and numbers 123.</p>\n",
    "    \"\"\"\n",
    "    clean_tokens = preprocess_text(sample_text)\n",
    "    print(\"âœ… Clean Tokens:\", clean_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
